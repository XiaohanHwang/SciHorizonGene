{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24c0e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this notebook, we will introduce how to start the evaluation.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this notebook, we will introduce how to start the evaluation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d35a01",
   "metadata": {},
   "source": [
    "Chapter 1: Question set structure\\\n",
    "In this chapter, we will introduce how to load the question set and the structure of the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45fd7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75 questions.\n",
      "question_index: 6648c31cda62de46659828790ca679fd4088153f24f8851e52157f5a83bce473\n",
      "question_type: single_choice\n",
      "knowledge: chromosome\n",
      "question: Select the chromosome location of LOC124905048 gene.\n",
      "options: {'A': '15;15q15.2', 'B': 'X;Xq11-q12', 'C': 'X;Xp11.3-p11.23', 'D': '21;21q21.1'}\n",
      "answer: D\n",
      "batch_id: 3\n",
      "perspective: study_bias\n",
      "flag: loc\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open(\"../dataset/questions_sample_5.pkl\", \"rb\") as f:\n",
    "    question_data = pkl.load(f)\n",
    "print(f\"Loaded {len(question_data)} questions.\")\n",
    "for k,v in question_data[0].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023fcb0",
   "metadata": {},
   "source": [
    "Take the first question in the question set as an example:\n",
    "\n",
    "``` the unique identifier of the question ```\\\n",
    "question_index: 6648c31cda62de46659828790ca679fd4088153f24f8851e52157f5a83bce473\n",
    "\n",
    "``` the type of the question, it determines which prompt template to use ```\\\n",
    "question_type: single_choice\n",
    "\n",
    "``` the knowledge domain of the question, corresponding to 8 gene knowledge domains in our paper ```\\\n",
    "knowledge: chromosome\n",
    "\n",
    "``` the question text ```\\\n",
    "question: Select the chromosome location of LOC124905048 gene.\n",
    "options: {'A': '15;15q15.2', 'B': 'X;Xq11-q12', 'C': 'X;Xp11.3-p11.23', 'D': '21;21q21.1'}\n",
    "\n",
    "``` the correct answer ```\\\n",
    "answer: D\n",
    "\n",
    "``` the batch id of the question, we use it in our preliminary experiment, you can ignore it ```\\\n",
    "batch_id: 3\n",
    "\n",
    "``` the perspective of LLMs' capability, which is corresponding to 4 perspectives in our paper ```\\\n",
    "perspective: study_bias\n",
    "\n",
    "``` the flag to tag whether the gene is a LOC-prefixed gene or not ```\\\n",
    "flag: loc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66f49c",
   "metadata": {},
   "source": [
    "Chapter 2: Load the model\\\n",
    "In this chapter, we load the model we want to evaluate. We recommend using OpenAI compatible interface.\\\n",
    "Here we use ```langchain``` framework to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f607f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_127650/468090166.py:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  client = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you for asking! ðŸ˜Š I'm here and ready to help you with anything you needâ€”whether it's answering questions, brainstorming ideas, or just having a friendly chat. How are you doing today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# set your model, api base and api key here\n",
    "model_name = \"\"\n",
    "api_base = \"\"\n",
    "api_key = \"\"\n",
    "\n",
    "client = ChatOpenAI(\n",
    "            model= f\"{model_name}\",\n",
    "            openai_api_base=api_base, \n",
    "            openai_api_key=api_key,    \n",
    "            temperature=0,     #  setting temperature to 0 for reproducibility\n",
    "        )\n",
    "\n",
    "# test the model status\n",
    "message = \"Hello, how are you?\"\n",
    "human_message = [HumanMessage(content=message)]\n",
    "response = client.invoke(human_message)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0157f",
   "metadata": {},
   "source": [
    "Chapter 3: Using our prompt template to evaluate the model\\\n",
    "In this chapter, we will use our prompt template and question to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee0a18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"answer\": \"C\"\n",
      "}\n",
      "```\n",
      "The correct answer is: C\n"
     ]
    }
   ],
   "source": [
    "from prompt_pydantic import get_prompt, get_parser_by_type\n",
    "# take the first question as an example\n",
    "question_content = get_prompt(question_data[5])\n",
    "parser = get_parser_by_type(question_data[5][\"question_type\"])\n",
    "prompt_str = question_content + \"\\n\" + parser.get_format_instructions()\n",
    "# wrap into HumanMessage\n",
    "messages = [HumanMessage(content=prompt_str)]\n",
    "# evaluate the model and compare the answer\n",
    "response = client.invoke(messages)\n",
    "print(response.content)\n",
    "print(\"The correct answer is: \" + question_data[5][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192731a",
   "metadata": {},
   "source": [
    "If you have get the response, you have successfully completed the model call.\\\n",
    "Then you can code by yourself to record the answer and calculate matrics. \\\n",
    "In the following chapter, we will introduce the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0838169",
   "metadata": {},
   "source": [
    "Chapter 4: Metrics\\\n",
    "In this chapter, we will introduce the metrics one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"answer\": \"C\"\n",
      "}\n",
      "```\n",
      "The correct answer is: C\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 4-1 single choice\n",
    "from metrics.choice import calculate_single_choice\n",
    "\n",
    "question_content = get_prompt(question_data[5])\n",
    "parser = get_parser_by_type(question_data[5][\"question_type\"])\n",
    "prompt_str = question_content + \"\\n\" + parser.get_format_instructions()\n",
    "messages = [HumanMessage(content=prompt_str)]\n",
    "response = client.invoke(messages)\n",
    "\n",
    "score = calculate_single_choice(question_data[5][\"answer\"], response.content)\n",
    "print(response.content)\n",
    "print(\"The correct answer is: \" + question_data[5][\"answer\"])\n",
    "print(f\"Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835ed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"answers\": [\"A\", \"B\", \"C\", \"D\"]\n",
      "}\n",
      "```\n",
      "The correct answer is: ['D', 'A', 'B', 'C']\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 4-2 multiple choice\n",
    "import importlib\n",
    "import metrics.choice\n",
    "importlib.reload(metrics.choice)\n",
    "from evaluate.metrics.choice import calculate_multiple_choice\n",
    "\n",
    "\n",
    "question_content = get_prompt(question_data[48])\n",
    "parser = get_parser_by_type(question_data[48][\"question_type\"])\n",
    "prompt_str = question_content + \"\\n\" + parser.get_format_instructions()\n",
    "messages = [HumanMessage(content=prompt_str)]\n",
    "response = client.invoke(messages)\n",
    "\n",
    "\n",
    "score = calculate_multiple_choice(question_data[48][\"answer\"], response.content)\n",
    "print(response.content)\n",
    "print(\"The correct answer is: \" + str(question_data[48][\"answer\"]))\n",
    "print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19b6b58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Tissue\": [\"liver\", \"adrenal\", \"pancreas\", \"fat\", \"testis\", \"ovary\", \"prostate\", \"colon\", \"small intestine\", \"duodenum\", \"stomach\", \"esophagus\", \"lung\", \"thyroid\", \"gall bladder\", \"kidney\", \"urinary bladder\", \"endometrium\", \"placenta\", \"skin\", \"salivary gland\", \"brain\", \"spleen\", \"lymph node\", \"appendix\", \"bone marrow\", \"heart\"], \"Category\": \"Ubiquitous expression\"}\n",
      "The correct answer is: {'tissue_list': ['liver', 'kidney', 'skin', 'brain', 'fat', 'duodenum'], 'category': 'Biased expression'}\n",
      "Score: 0.1818181818181818\n"
     ]
    }
   ],
   "source": [
    "# 4-3 expression\n",
    "import importlib\n",
    "importlib.reload(metrics.expression)\n",
    "from metrics.expression import calculate_expression\n",
    "\n",
    "ind = 74\n",
    "question_content = get_prompt(question_data[ind])\n",
    "parser = get_parser_by_type(question_data[ind][\"question_type\"])\n",
    "\n",
    "prompt_str = question_content + \"\\n\" + parser.get_format_instructions()\n",
    "messages = [HumanMessage(content=prompt_str)]\n",
    "response = client.invoke(messages)\n",
    "\n",
    "\n",
    "score = calculate_expression(question_data[ind][\"answer\"], response.content)\n",
    "print(response.content)\n",
    "print(\"The correct answer is: \" + str(question_data[ind][\"answer\"]))\n",
    "print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f9e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huangxiaohan/SciHorizonGene/evaluate/metrics/go-basic.obo: fmt(1.2) rel(2025-10-10) 42,666 Terms\n",
      "[\n",
      "    {\"go\": \"mitochondrial respiratory chain complex I assembly\", \"evidence\": \"IMP\"},\n",
      "    {\"go\": \"mitochondrial respiratory chain complex I biogenesis\", \"evidence\": \"IMP\"},\n",
      "    {\"go\": \"mitochondrial respiratory chain complex I assembly\", \"evidence\": \"IDA\"},\n",
      "    {\"go\": \"mitochondrial respiratory chain complex I biogenesis\", \"evidence\": \"IDA\"},\n",
      "    {\"go\": \"mitochondrial inner membrane\", \"evidence\": \"IDA\"},\n",
      "    {\"go\": \"mitochondrial respiratory chain complex I\", \"evidence\": \"IDA\"},\n",
      "    {\"go\": \"mitochondrial respiratory chain complex I\", \"evidence\": \"IEA\"},\n",
      "    {\"go\": \"mitochondrial inner membrane\", \"evidence\": \"IEA\"},\n",
      "    {\"go\": \"integral component of mitochondrial inner membrane\", \"evidence\": \"IEA\"},\n",
      "    {\"go\": \"NADH dehydrogenase (ubiquinone) activity\", \"evidence\": \"IEA\"},\n",
      "    {\"go\": \"oxidoreduction-driven active transmembrane transporter activity\", \"evidence\": \"IEA\"},\n",
      "    {\"go\": \"proton transmembrane transport\", \"evidence\": \"IEA\"}\n",
      "]\n",
      "The correct answer is: [{'go': 'enables protein binding', 'evidence': 'IPI'}, {'go': 'located_in mitochondrion', 'evidence': 'HTP'}, {'go': 'located_in mitochondrion', 'evidence': 'IDA'}, {'go': 'located_in mitochondrion', 'evidence': 'IEA'}, {'go': 'located_in mitochondrial inner membrane', 'evidence': 'IEA'}, {'go': 'located_in mitochondrial inner membrane', 'evidence': 'TAS'}, {'go': 'located_in membrane', 'evidence': 'IEA'}, {'go': 'part_of respiratory chain complex I', 'evidence': 'IBA'}, {'go': 'located_in mitochondrial inner membrane', 'evidence': 'IDA'}, {'go': 'involved_in aerobic respiration', 'evidence': 'NAS'}, {'go': 'involved_in proton motive force-driven mitochondrial ATP synthesis', 'evidence': 'NAS'}, {'go': 'part_of respiratory chain complex I', 'evidence': 'IDA'}]\n",
      "Precision: 0.2, Recall: 0.1076923076923077, F1: 0.14, Hallucination Rate: 0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "# 4-4 gene ontology\n",
    "import importlib\n",
    "importlib.reload(metrics.ontology)\n",
    "from metrics.ontology import calculate_go\n",
    "\n",
    "\n",
    "ind = 54\n",
    "question_content = get_prompt(question_data[ind])\n",
    "parser = get_parser_by_type(question_data[ind][\"question_type\"])\n",
    "\n",
    "prompt_str = question_content + \"\\n\" + parser.get_format_instructions()\n",
    "messages = [HumanMessage(content=prompt_str)]\n",
    "response = client.invoke(messages)\n",
    "\n",
    "\n",
    "score = calculate_go(question_data[ind][\"answer\"], response.content)\n",
    "print(response.content)\n",
    "print(\"The correct answer is: \" + str(question_data[ind][\"answer\"]))\n",
    "print(f\"Precision: {score[0]}, Recall: {score[1]}, F1: {score[2]}, Hallucination Rate: {score[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afc5f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"summary\": \"MIR151A is a microRNA gene that regulates gene expression post-transcriptionally by binding to target mRNAs, often involved in processes such as cell proliferation, differentiation, and immune response.\"\n",
      "}\n",
      "```\n",
      "The correct answer is: microRNAs (miRNAs) are short (20-24 nt) non-coding RNAs that are involved in post-transcriptional regulation of gene expression in multicellular organisms by affecting both the stability and translation of mRNAs. miRNAs are transcribed by RNA polymerase II as part of capped and polyadenylated primary transcripts (pri-miRNAs) that can be either protein-coding or non-coding. The primary transcript is cleaved by the Drosha ribonuclease III enzyme to produce an approximately 70-nt stem-loop precursor miRNA (pre-miRNA), which is further cleaved by the cytoplasmic Dicer ribonuclease to generate the mature miRNA and antisense miRNA star (miRNA*) products. The mature miRNA is incorporated into a RNA-induced silencing complex (RISC), which recognizes target mRNAs through imperfect base pairing with the miRNA and most commonly results in translational inhibition or destabilization of the target mRNA. The RefSeq represents the predicted microRNA stem-loop.\n",
      "Rouge Score: 0.12716763005780346, BERT F1: 0.5477575659751892, Perplexity: 24.460006713867188, Length: 41\n"
     ]
    }
   ],
   "source": [
    "# 4-5 summary\n",
    "import importlib\n",
    "importlib.reload(metrics.summary)\n",
    "from metrics.summary import calculate_summary\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ind = 60\n",
    "question_content = get_prompt(question_data[ind])\n",
    "parser = get_parser_by_type(question_data[ind][\"question_type\"])\n",
    "\n",
    "prompt_str = question_content + \"\\n\" + parser.get_format_instructions()\n",
    "messages = [HumanMessage(content=prompt_str)]\n",
    "response = client.invoke(messages)\n",
    "\n",
    "\n",
    "score = calculate_summary(question_data[ind][\"answer\"], response.content)\n",
    "print(response.content)\n",
    "print(\"The correct answer is: \" + str(question_data[ind][\"answer\"]))\n",
    "print(f\"Rouge Score: {score[0]}, BERT F1: {score[1]}, Perplexity: {score[2]}, Length: {score[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02145ea0",
   "metadata": {},
   "source": [
    "Congratulations! You have finished the evaluation progress. :) \n",
    "\n",
    "If you counter any Path Error, please check the path of the files. \\\n",
    "Attention:\\\n",
    "When you run summary evaluation, you will download the gpt-2 model.\n",
    "\n",
    "At last, you can:\n",
    "1. Filter the results by the perspective (e.g.  ```study bias a.k.a. Research Attention```), question type (e.g. ```single-choice``` ) and tag (loc, non-loc) to reproduce the ```Research Attention``` experiment.\n",
    "2. Filter the results by the perspective (e.g. ```hallucination```) to reproduce the ```Hallucination``` experiment.\n",
    "3. Filter the results by the perspective (e.g. ```completeness```) to reproduce the ```Knowledge Completness``` experiment.\n",
    "4. Filter the results by the perspective (e.g. ```literature_utilize```) to reproduce the ```Literature Influence``` experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biodata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
